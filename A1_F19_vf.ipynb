{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fzMyE386oaaV"
   },
   "source": [
    "**Fall 2019**\n",
    "\n",
    "**P556: Applied Machine Learning**\n",
    "\n",
    "**Assignment #1**\n",
    "\n",
    "**Due date: September 18, 2019. 11:59 PM**\n",
    "\n",
    "DO NOT CHANGE THE FUNCTION DEFINITIONS UNLESS APPROVED BY AN AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJz8ZxU7opMy"
   },
   "source": [
    "# Problem #1: Linear Regression\n",
    "\n",
    "##  Problem 1.1 (25 points)\n",
    "\n",
    "Implement linear regression using gradient descent. Your implementation should be able to handle simple and multiple linear regression.\n",
    "\n",
    "Note 1: by implementation we mean that everything has to be written from scratch and that you cannot call a linear regression function from a library, such as sklearn. Usage of standard libraries, such as numpy, pandas, etc., is fine. If you are unsure about whether a library can be used, please contact the AIs well in advance of the submission date.\n",
    "\n",
    "Note 2: You are free to use sklearn to test whether your results match that from a battle-tested library. This is a great way to know before hand whether your submission is correct. Make sure to use the same parameters on both models before you spend an eternity debugging code that is correct but not returning the same values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dW1xPyXPoonO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as random\n",
    "random.seed(9001)\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from itertools import combinations \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class linear_regression:\n",
    "    def __init__(self, learning_rate, iterations, \n",
    "               fit_intercept=True, normalize=False, coef=None):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.normalize = normalize\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.coef = coef\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit linear model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array_like, shape (n_samples, n_targets)\n",
    "            Target values.\n",
    "\n",
    "        #DISCLAIMER: The inputs X & y must be passed as a DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        #Initialize Coefficients\n",
    "        global theta\n",
    "        theta = np.zeros((X.shape[1]+1,1)) #Initialize theta to the number of features + dummy variable fpr theta 0\n",
    "        m = X.shape[0] #Initialize the number of elements in the Dataset\n",
    "\n",
    "        alpha = self.learning_rate #Initialize Learning Rate\n",
    "        iters = self.iterations #Initialize number of Iterations\n",
    "\n",
    "        #Feature Normalization\n",
    "        if (self.normalize == True):\n",
    "            X_norm = X; \n",
    "            mu = np.mean(X_norm) #Compute Mean\n",
    "            sigma = np.std(X_norm,ddof=1) #Compute Standard Deviation\n",
    "\n",
    "            #Perform Normalization\n",
    "            X_norm = (X_norm - mu)/sigma\n",
    "            X = X_norm     \n",
    "\n",
    "        #Insert a Dummy variable for Theta 0\n",
    "        X.insert(0, '', np.ones(X.shape[0]))\n",
    "        cost = [] #Cost History variable\n",
    "\n",
    "        #Gradident Descent\n",
    "        for i in range(0,iters):\n",
    "            error = np.dot(np.squeeze(np.asarray(X)),theta) - np.array(pd.DataFrame(y))\n",
    "            cost.append((1/(2*m)) * np.square(error))\n",
    "            X_temp = np.dot(np.array(error.T),np.array(X))\n",
    "            theta = theta - (X_temp * (alpha/m)).T\n",
    "        return theta,cost\n",
    "    pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using the linear model\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array_like, shape (n_samples, n_features)\n",
    "            Samples.\n",
    "        Returns\n",
    "        -------\n",
    "        C : array, shape (n_samples,)\n",
    "            Returns predicted values.\n",
    "        \"\"\"\n",
    "        if (self.normalize == True):\n",
    "            #Feature Normalization\n",
    "            X_norm = X; \n",
    "            mu = np.mean(X_norm) #Compute Mean\n",
    "            sigma = np.std(X_norm,ddof=1) #Compute Standard Deviation\n",
    "            #Perform Normalization\n",
    "            X_norm = (X_norm - mu)/sigma #Normalize\n",
    "            X = X_norm \n",
    "\n",
    "        #Insert a Dummy variable for Theta 0\n",
    "        X.insert(0,'', np.ones(X.shape[0]))\n",
    "\n",
    "        coef = np.dot(np.asarray(X),theta)\n",
    "        return coef\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Boston Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['PRICE'] = data.target\n",
    "\n",
    "boston_X = df['RM']\n",
    "boston_y = pd.DataFrame(df['PRICE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing implementation on a 1 dimensional dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneD = df[['RM','PRICE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Predict  Actual    Sq_Diff\n",
      "0  24.462430    24.0   0.213842\n",
      "1  19.577888    18.9   0.459533\n",
      "2  19.620660    18.9   0.519351\n",
      "3  17.944005    14.5  11.861168\n",
      "4  16.113371    13.9   4.899010\n",
      "5  23.778081    18.4  28.923759\n",
      "6  20.159585    14.5  32.030904\n",
      "7  19.115953    13.2  34.998496\n",
      "8  18.183527    20.0   3.299575\n",
      "9  19.252822    24.7  29.671743\n",
      "\n",
      "The MSE for the above model: 49.946555682743494\n"
     ]
    }
   ],
   "source": [
    "#Using the feature RM, which represents number of rooms in a house, to predict the price\n",
    "oneD = df[['RM','PRICE']]\n",
    "\n",
    "#Create an instance of class Linear_Regression\n",
    "lin_reg = linear_regression\n",
    "model = lin_reg(0.01,1500,normalize=True)\n",
    "\n",
    "#Train & Test Split\n",
    "train = oneD.sample(frac=0.70, random_state=0)\n",
    "test = oneD.drop(train.index)\n",
    "\n",
    "train_X = train[['RM']]\n",
    "train_y = pd.DataFrame(train['PRICE'])\n",
    "test_X = test[['RM']]\n",
    "test_y = pd.DataFrame(test['PRICE'])\n",
    "\n",
    "coef, cost = model.fit(train_X,train_y)\n",
    "predictions = model.predict(test_X)\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "final_df = pd.DataFrame(predictions).reset_index(drop = True)\n",
    "final_df1 = pd.DataFrame(test_y).reset_index(drop = True)\n",
    "final_df['Actual'] = final_df1['PRICE']\n",
    "final_df = final_df.rename(columns={0:\"Predict\"})\n",
    "final_df['Sq_Diff'] = np.square(final_df['Actual'] - final_df['Predict'])\n",
    "MSE = np.sum(final_df['Sq_Diff'])/len(final_df)\n",
    "print(final_df.head(10))\n",
    "print(\"\\nThe MSE for the above model:\",MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the same against Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Predict  Actual    Sq_Diff  sklearn_Pred  Sq_Diff_sklearn\n",
      "0  24.462430    24.0   0.213842     25.041019         1.083721\n",
      "1  19.577888    18.9   0.459533     20.078959         1.389945\n",
      "2  19.620660    18.9   0.519351     20.122410         1.494286\n",
      "3  17.944005    14.5  11.861168     18.419146        15.359707\n",
      "4  16.113371    13.9   4.899010     16.559460         7.072728\n",
      "5  23.778081    18.4  28.923759     24.345809        35.352648\n",
      "6  20.159585    14.5  32.030904     20.669888        38.067515\n",
      "7  19.115953    13.2  34.998496     19.609693        41.084163\n",
      "8  18.183527    20.0   3.299575     18.662470         1.788988\n",
      "9  19.252822    24.7  29.671743     19.748735        24.515027\n",
      "\n",
      "The MSE from Sklearn is: 49.402477125845444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# instantiate, fit & predict\n",
    "lm2 = LinearRegression()\n",
    "lm2.fit(train_X, train_y)\n",
    "\n",
    "# predict for a new observation\n",
    "final_df['sklearn_Pred'] = lm2.predict(test_X)\n",
    "\n",
    "#Put the same in a Dataframe for output\n",
    "final_df['Sq_Diff_sklearn'] = np.square(final_df['Actual'] - final_df['sklearn_Pred'])\n",
    "MSE = np.sum(final_df['Sq_Diff_sklearn'])/len(final_df)\n",
    "print(final_df.head(10))\n",
    "print(\"\\nThe MSE from Sklearn is:\",MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing implementation on n dimensional dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiD = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Predict  Actual    Sq_Diff\n",
      "0  29.257814    24.0  27.644613\n",
      "1  18.416005    18.9   0.234251\n",
      "2  20.881352    18.9   3.925754\n",
      "3  14.126827    14.5   0.139258\n",
      "4  13.304788    13.9   0.354278\n",
      "5  19.579230    18.4   1.390583\n",
      "6  17.974116    14.5  12.069481\n",
      "7   9.246141    13.2  15.633002\n",
      "8  21.880027    20.0   3.534503\n",
      "9  22.521078    24.7   4.747701\n",
      "\n",
      "The MSE for the above model: 24.577718522868278\n"
     ]
    }
   ],
   "source": [
    "#Create an instance of class Linear_Regression\n",
    "lin_reg = linear_regression\n",
    "model = lin_reg(0.01,1500,normalize=True)\n",
    "\n",
    "#Train & Test Split\n",
    "train = multiD.sample(frac=0.70, random_state=0)\n",
    "test = multiD.drop(train.index)\n",
    "\n",
    "train_X = train[['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT']]\n",
    "train_y = pd.DataFrame(train['PRICE'])\n",
    "\n",
    "test_X = test[['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT']]\n",
    "test_y = pd.DataFrame(test['PRICE'])\n",
    "\n",
    "coef, cost = model.fit(train_X,train_y)\n",
    "predictions = model.predict(test_X)\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "final_df = pd.DataFrame(predictions).reset_index(drop = True)\n",
    "final_df1 = pd.DataFrame(test_y).reset_index(drop = True)\n",
    "final_df['Actual'] = final_df1['PRICE']\n",
    "final_df = final_df.rename(columns={0:\"Predict\"})\n",
    "final_df['Sq_Diff'] = np.square(final_df['Actual'] - final_df['Predict'])\n",
    "MSE = np.sum(final_df['Sq_Diff'])/len(final_df)\n",
    "print(final_df.head(10))\n",
    "print(\"\\nThe MSE for the above model:\",MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the same against Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Predict  Actual    Sq_Diff  sklearn_Pred  Sq_Diff_sklearn\n",
      "0  29.257814    24.0  27.644613     29.601692        31.378950\n",
      "1  18.416005    18.9   0.234251     19.375095         0.225715\n",
      "2  20.881352    18.9   3.925754     21.904876         9.029281\n",
      "3  14.126827    14.5   0.139258     14.318240         0.033037\n",
      "4  13.304788    13.9   0.354278     13.843175         0.003229\n",
      "5  19.579230    18.4   1.390583     19.982648         2.504774\n",
      "6  17.974116    14.5  12.069481     18.439118        15.516650\n",
      "7   9.246141    13.2  15.633002      9.455626        14.020339\n",
      "8  21.880027    20.0   3.534503     22.617710         6.852406\n",
      "9  22.521078    24.7   4.747701     23.250711         2.100438\n",
      "\n",
      "The MSE from Sklearn is: 24.54141609099559\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# instantiate, fit & predict\n",
    "lm2 = LinearRegression()\n",
    "lm2.fit(train_X, train_y)\n",
    "\n",
    "# predict for a new observation\n",
    "final_df['sklearn_Pred'] = lm2.predict(test_X)\n",
    "\n",
    "#Put the same in a Dataframe for output\n",
    "final_df['Sq_Diff_sklearn'] = np.square(final_df['Actual'] - final_df['sklearn_Pred'])\n",
    "MSE = np.sum(final_df['Sq_Diff_sklearn'])/len(final_df)\n",
    "print(final_df.head(10))\n",
    "print(\"\\nThe MSE from Sklearn is:\",MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Man3c1JrhVbr"
   },
   "source": [
    "## Problem 1.2 (10 points)\n",
    "\n",
    "- Split the Boston Housing dataset into train and test sets (70% and 30%, respectively) (5 points). \n",
    "- Fit your linear regression implementation using the training set and print your model's coefficients. Make predictions for the test set using your fitted model (5 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEFBL6WwhXUz",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Predict  Actual    Sq_Diff\n",
      "0  29.420354    24.0  29.380232\n",
      "1  18.644164    18.9   0.065452\n",
      "2  21.048841    18.9   4.617520\n",
      "3  14.247039    14.5   0.063989\n",
      "4  13.416465    13.9   0.233806\n",
      "5  19.796832    18.4   1.951139\n",
      "6  18.096802    14.5  12.936985\n",
      "7   9.404868    13.2  14.403030\n",
      "8  21.731641    20.0   2.998582\n",
      "9  22.493026    24.7   4.870736\n",
      "\n",
      "The MSE for the above model: 24.81261326165304\n",
      "\n",
      " The Coefficients are [[22.35298975]\n",
      " [-0.77023571]\n",
      " [ 0.51711781]\n",
      " [-0.22459182]\n",
      " [ 0.36938683]\n",
      " [-1.64383473]\n",
      " [ 2.91489359]\n",
      " [-0.08906462]\n",
      " [-2.51027883]\n",
      " [ 1.69166066]\n",
      " [-1.05438226]\n",
      " [-1.87855168]\n",
      " [ 0.95352462]\n",
      " [-3.42617772]]\n"
     ]
    }
   ],
   "source": [
    "#Create an instance of class Linear_Regression\n",
    "lin_reg = linear_regression\n",
    "model = lin_reg(0.01,1000,normalize=True)\n",
    "\n",
    "#Train & Test Split\n",
    "train = multiD.sample(frac=0.70, random_state=0)\n",
    "test = multiD.drop(train.index)\n",
    "\n",
    "train_X = train[['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT']]\n",
    "train_y = pd.DataFrame(train['PRICE'])\n",
    "\n",
    "test_X = test[['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT']]\n",
    "test_y = pd.DataFrame(test['PRICE'])\n",
    "\n",
    "coef, cost = model.fit(train_X,train_y)\n",
    "predictions = model.predict(test_X)\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "final_df = pd.DataFrame(predictions).reset_index(drop = True)\n",
    "final_df1 = pd.DataFrame(test_y).reset_index(drop = True)\n",
    "final_df['Actual'] = final_df1['PRICE']\n",
    "final_df = final_df.rename(columns={0:\"Predict\"})\n",
    "final_df['Sq_Diff'] = np.square(final_df['Actual'] - final_df['Predict'])\n",
    "MSE = np.sum(final_df['Sq_Diff'])/len(final_df)\n",
    "print(final_df.head(10))\n",
    "print(\"\\nThe MSE for the above model:\",MSE)\n",
    "print(\"\\n The Coefficients are\",coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FXqtD_KhZwV"
   },
   "source": [
    "## Problem 1.3 (10 points)\n",
    "\n",
    "Identify the variable or set of variables that will minimize the mean square error (MSE). Hint: this is where your function being able to handle simple and multiple regression becomes useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#All column names\n",
    "choices = data.feature_names\n",
    "\n",
    "#Empty lists to \n",
    "feature = []\n",
    "score = []\n",
    "\n",
    "#Defining model to iterate combinations of features over\n",
    "lin_reg = linear_regression\n",
    "model = lin_reg(0.01,150,normalize=True)\n",
    "\n",
    "for r in range(2,14):\n",
    "    feature_combo = list(combinations(choices, r))\n",
    "    for i in range(0,len(feature_combo)):   \n",
    "#         print(feature_combo[i])\n",
    "\n",
    "        #Train & Test Split\n",
    "        train = df.sample(frac=0.70, random_state=0)\n",
    "        test = df.drop(train.index)\n",
    "\n",
    "        train_X = train[list(feature_combo[i])]\n",
    "        train_y = train['PRICE']\n",
    "        test_X = test[list(feature_combo[i])]\n",
    "        test_y = test['PRICE']\n",
    "\n",
    "        #Fit and Predict\n",
    "        coeffs = model.fit(train_X,train_y)\n",
    "        predictions = list(model.predict(test_X))\n",
    "\n",
    "        final_df = pd.DataFrame()\n",
    "        final_df = pd.DataFrame(predictions).reset_index(drop = True)\n",
    "        final_df1 = pd.DataFrame(test_y).reset_index(drop = True)\n",
    "        final_df['Actual'] = final_df1['PRICE']\n",
    "        final_df = final_df.rename(columns={0:\"Predict\"})\n",
    "        final_df['Sq_Diff'] = np.square(final_df['Actual'] - final_df['Predict'])\n",
    "        MSE = np.sum(final_df['Sq_Diff'])/len(final_df)\n",
    "        print(MSE)\n",
    "\n",
    "        feature.append(feature_combo[i])\n",
    "        score.append(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the results to a csv and reading the same\n",
    "MSE_Eval = pd.DataFrame(np.asarray(feature))\n",
    "MSE_Eval['MSE'] = np.asarray(score)\n",
    "MSE_Eval.to_csv('MSE_Eval.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_Eval = pd.read_csv('MSE_Eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The least Mean Square Error obtained over 150 iterations: 59.14562072841089\n",
      "The variables include ('CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'PTRATIO', 'B', 'LSTAT')\n"
     ]
    }
   ],
   "source": [
    "#Finding the index of the least MSE\n",
    "index = MSE_Eval[(MSE_Eval['MSE'] == np.min(MSE_Eval['MSE']))].reset_index()['index'][0]\n",
    "print(\"The least Mean Square Error obtained over 150 iterations:\",np.min(MSE_Eval['MSE']))\n",
    "print(\"The variables include\",MSE_Eval['0'][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__:\n",
    "The least value for MSE obtained over 1500 iterations as shown previously is 24.81. However, the final result of the combination of variables that lead to the least MSE seems to depend on factors like number of iterations, learning rate. \n",
    "\n",
    "In this case, over 150 iterations, the least MSE that could be obtained was 59.15 using the features mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KcqYa1U3iRQ1"
   },
   "source": [
    "## Problem 1.4 (5 points)\n",
    "\n",
    "1. __How do you interpret that a variable causes a model's mean square error to increase? (2 points)__\n",
    "  - Answer: Mean square error gives us an idea of how close the predictions are to the actuals. There can be number of factors that lead to increase in MSE of a model. Some of them are:\n",
    "      * __Outliers__ - If there are outliers present in handful of variables, they can cause the model to perform poorly by skewing the prediction in the wrong direction.      \n",
    "      \n",
    "      * __Autocorrelation__ - Occurs when the instances of a variable are correlated with itself. Presence of this would lead the model to predict something for y(x+n) based on y(x) when no real relationship exists. \n",
    "      \n",
    "      * __Underfitting__ - Not including variables that explain the target variable better in the model will cause an increase in MSE. Basically absence of compleexity needed to explain the data.\n",
    "      \n",
    "      * __Scale of features__ - Not normalizing the data may sometimes cause the model to ignore a variable or two just because it is on a smaller scale\n",
    "  \n",
    "2. __Why we would want to normalize our variables? (1 point)__\n",
    "  - Answer:The goal of normalization is to have all the data points on the same scale so that each feature is equally important. This way we do not introduce a bias in the model. \n",
    "  \n",
    "  Ex: Consider age of the house, number of rooms as the features used in a model.Without normalization, the age can range anywhere between 1 to say a 100 and rooms usually tend to range between 1 to say 10. This would be telling the model, one feature is more important the other when it is not the case\n",
    "  \n",
    "  \n",
    "3. __A model fitted using the exact same split dataset with normalized values will generate the same coefficients as a model that was fitted using values that haven't been normalized. Clearly state whether that statement is true or false and explain your reasoning. (2 points)__\n",
    "  - Answer: The statement is false. As mentioned above, having features in different scales causes addition of unnecessary and inaccurate bias into the model. The weightage of each feature will be predetermined when they are not normalized and may end up ignoring an important feature just because it was on a smaller scale.\n",
    "  \n",
    "  Also, the coefficients of the model fitted with and without normalization cannot be the same due to the reasons elucidated above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJSot41BkMrB"
   },
   "source": [
    "# Problem 2: Binary Classification\n",
    "\n",
    "## Problem 2.1 (5 points)\n",
    "\n",
    "Consider the binary classification problem of mapping a given input to two classes. Let $\\mathcal{X}=\\mathbb{R}^d$ and $\\mathcal{Y}=\\{+1, -1\\}$ be the input space and output space, respectively. In simple words, it means that the input has $d$ features and all of them are real valued, whereas the output can only take values $-1$ or $+1$. This is one of the most common problems in machine learning and many sophisticated methods exist to solve it. In the question, we will solve it using the concepts we have already learned in class. Let us assume the two sets of points can be separated using a straight line i.e. the samples are linearly separable. So if $d=2$, one should be able to draw a line to distinguish between the two classes. All points lying on side of the line should belong to a particular class (say $1$) and the points lying on the other side should belong to another class (say $2$). To see what this would look like,  your first task is as follows:\n",
    "\n",
    "Write a function that will randomly generate a dataset for this problem. Your function should randomly choose a line $l$, which can be denoted as $ax + by + c = 0$. According to basic high school geometry, the line divides the plane into two sides. On one side, $ax+by+c>0$ while on the other $ax+by+c<0$. Use this fact to randomly generate $k_0$ points on the side of class 0 (i.e. $y=-1$) and $k_1$ points on the side of class 1 (i.e. $y=1$). Create a plot of this dataset where all the points corresponding to one class are blue and those of the other class are green, the line dividing both classes should be red. Axes should be labeled.\n",
    "\n",
    "**Note**: Do not confuse the $x$ and $y$ in the equation of line $ax + by + c = 0$ with $\\mathcal{X} $ and $\\mathcal{Y}$. Instead imagine these $x$ and $y$ as the 2-D coordinate system on which you have different points which should lie on 2 sides of the line $ax + by + c = 0$. For example, there is a point (2,3) in the 2-D system where $x = 2$ and $y = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g96jFpGyMIFu"
   },
   "outputs": [],
   "source": [
    "def generate_dataset(k0, k1):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    k0 : integer, number of samples for class 0\n",
    "    k1 : integer, number of samples for class 1\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : array, shape (m, d), dimension numpy array where m is the number of \n",
    "    samples and d is the number of features \n",
    "\n",
    "    Y : array, (m, 1), dimension vector where m is the number of samples\n",
    "    \"\"\"\n",
    "        \n",
    "    #Generating 3 random numbers for a, b & c, considering the line \n",
    "    #in the form ax+by+c = 0\n",
    "    a = random.random()*100\n",
    "    b = random.random()*100\n",
    "    c = random.random()*100\n",
    "\n",
    "    #Generating random data points for x1,y1\n",
    "    x1 = np.random.uniform(100,-100,k0)\n",
    "    y1 = ((-c-(a*x1))/b) + np.random.randint(10,100,size = k0)\n",
    "\n",
    "    #Generating random data points for x2,y2\n",
    "    x2 = np.random.uniform(100,-100,k1)\n",
    "    y2 = ((-c-(a*x2))/b) - np.random.randint(10,100,size = k1)\n",
    "\n",
    "    \n",
    "    #Plotting the same\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    #Enabling 4 quadrant view\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    #Defining the graph\n",
    "    ax.spines['left'].set_position('center')\n",
    "    ax.spines['bottom'].set_position('center')\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "   \n",
    "    plt.plot(x1, y1,'b.',label='Points to the Left')\n",
    "    plt.plot(x2, y2,'g.',label='Points to the Right')\n",
    "\n",
    "    x=list(x1) + list(x2)\n",
    "    x = np.asarray(x)\n",
    "    y = (-c-(a*x))/b\n",
    "    plt.plot(x,y,'r-')\n",
    "    plt.plot(x,y,'r-',label='Separator')\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    #Join the x1,y1 and x2,y2 into one array to be returned from the function\n",
    "    temp = pd.DataFrame()\n",
    "    temp2 = pd.DataFrame()\n",
    "    temp['x1'] = x1\n",
    "    temp['x2'] = y1\n",
    "    temp['y'] = -1\n",
    "\n",
    "    temp2['x1'] = x2\n",
    "    temp2['x2'] = y2\n",
    "    temp2['y'] = 1\n",
    "\n",
    "    temp = temp.append(temp2)\n",
    "    X = np.asarray(temp[['x1','x2']])\n",
    "    Y = np.asarray(temp['y'])\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_x,data_y = generate_dataset(20,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9mw15CFikXI1"
   },
   "source": [
    "## Problem 2.2 (35 points)\n",
    "\n",
    "If $\\mathcal{Y}$ is the variable you are trying to predict using a feature $\\mathcal{X}$ then in a typical Machine Learning problem, you are tasked with a target function $f$ which maps $\\mathcal{X}$ to $\\mathcal{Y}$ i.e. Find $f$ such that  $\\mathcal{Y}$  = $f(\\mathcal{X})$\n",
    "\n",
    "\n",
    "When you are given a dataset for which you do not have access to the target function $f$, you have to learn it from the data. In this problem, we are going to learn the parameters of the line that separates the two classes for the dataset that we constructed in Problem 2.1. As we previously mentioned, that line can be represented as $ax + by + c = 0$.\n",
    "\n",
    "The goal here is to correctly find out the coefficients $a$, $b$, and $c$, represented below as $\\bf{w}$ which is a vector. The algorithm to find it is a simple iterative process: \n",
    "\n",
    "1. Randomly choose a $\\mathbf{w}$ to begin with.\n",
    "2. Keep on adjusting the value of $\\bf{w}$ as follows until all data samples are correctly classified:\n",
    "    1. Randomly choose a sample from the dataset without replacement and see if it is correctly classified. If yes,  move on to another sample.\n",
    "    2. If not,  then  update the weights as $\\mathbf{w}^{t+1} = \\mathbf{w}^t + y \\cdot \\mathbf{x}$\n",
    "    and go back to the previous step (of randomly chosing a sample)\n",
    "    \n",
    "        - $\\mathbf{w}^{t+1}$ is value of $\\mathbf{w}$ at iteration $t+1$\n",
    "        - $\\mathbf{w}^{t}$ is value of $\\mathbf{w}$ at iteration $t$\n",
    "        - $y$ is the class label for the sample under consideration\n",
    "        - $\\mathbf{x}$ is the data-point under consideration\n",
    "    \n",
    "    \n",
    "Write a function that implements this learning algorithm. The input to the function is going to be a dataset represented by the input variable $X$ and the target variable $y$. The output of the function should be the chosen $\\mathbf{w}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function: Given w, returns 2 points on the line (the x & y intercept)\n",
    "def return_ends(a,b,c):\n",
    "    #ax + by + c = 0\n",
    "    #the x intercept\n",
    "    x_int = (-c/a,0)\n",
    "    y_int = (0,-c/b)\n",
    "    return x_int,y_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_line(x, y):\n",
    "    \"\"\"Predict using the binary classification model. Use the dataset generated \n",
    "    using generate_data() as input for this function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like, shape (n_samples, n_features)\n",
    "        Samples.\n",
    "    y : array_like, shape (n_labels, 1)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : array, shape (1,n_features)\n",
    "        Returns the final weight vector w.  \n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.append(x,np.asarray(pd.DataFrame(np.ones(len(x)))),axis = 1)\n",
    "\n",
    "    w = np.random.randint(0,100,3)\n",
    "    success = 0\n",
    "\n",
    "    while success < len(x):\n",
    "        success = 0\n",
    "        for i in range(0,len(x)):\n",
    "            if ((w.dot(x[i]) > 0) & (y[i] == 1)):\n",
    "                success+=1\n",
    "            elif ((w.dot(x[i]) < 0) & (y[i] == -1)):\n",
    "                success+=1\n",
    "            else:\n",
    "                w = w+(y[i]*x[i])\n",
    "\n",
    "    #Create a plt object\n",
    "    fig = plt.figure()\n",
    "\n",
    "    #Enabling 4 quadrant view\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    #Defining the graph\n",
    "    ax.spines['left'].set_position('center')\n",
    "    ax.spines['bottom'].set_position('center')\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "\n",
    "\n",
    "    #Plot the data points belinging to -1 and 1\n",
    "    temp = pd.DataFrame(data_x)\n",
    "    temp = temp.rename(columns={0: \"x1\", 1: \"x2\"})\n",
    "    temp['Y'] = pd.DataFrame(data_y)\n",
    "    left = temp[(temp['Y']== -1)][['x1','x2']]\n",
    "    right = temp[(temp['Y'] == 1)][['x1','x2']]\n",
    "\n",
    "    plt.plot(left['x1'],left['x2'],'b.',label='Points to the Left')\n",
    "    plt.plot(right['x1'],right['x2'],'g.',label='Points to the Right')\n",
    "    \n",
    "    #Plot the line derived from the algorithm\n",
    "    x1,y1 = return_ends(w[0],w[1],w[2])[0][0],0\n",
    "    x2,y2 = 0,return_ends(w[0],w[1],w[2])[1][1]\n",
    "\n",
    "    x_int = []\n",
    "    y_int = []\n",
    "    x_int.append(x1)\n",
    "    x_int.append(x2)\n",
    "\n",
    "    y_int.append(y1)\n",
    "    y_int.append(y2)\n",
    "\n",
    "    #Finding more points on the line so that the line is visible\n",
    "    rise = (return_ends(w[0],w[1],w[2])[1][1] - return_ends(w[0],w[1],w[2])[1][0])\n",
    "    run = (return_ends(w[0],w[1],w[2])[0][0] - return_ends(w[0],w[1],w[2])[0][1])\n",
    "    slope = rise/run\n",
    "    intercept = return_ends(w[0],w[1],w[2])[1][1]\n",
    "\n",
    "    #The equation of the line\n",
    "    #y = mx+b\n",
    "    y_points = []\n",
    "    for i in range(-100,100):    \n",
    "        y_points.append(slope *i + intercept)\n",
    "\n",
    "    x_points = []\n",
    "    for i in range(0,len(y_points)): \n",
    "        x_points.append((y_points[i] - intercept)/slope)\n",
    "\n",
    "\n",
    "    plt.plot(x_points,y_points,'r-',label = 'Separator')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = fit_line(data_x,data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHc3t4e9MOxu"
   },
   "source": [
    "### Problem 2.3 (10 points)\n",
    "- __Give an intuition of why the above algorithm converges for linearly separable data? We do not expect you to give a mathematic proof, but it would be great if you can provide one. You will get full points even if you just provide an intuition of a few lines. Including figures or mathematical equations is encouraged but not required. (5 points)__\n",
    "\n",
    "Answer: The above algorithm basically implements gradient descent. It converges because it is corrected to move in the right direction until it classifies nearly most of the data points accuratley.\n",
    "  - The classes -1 and 1, part of target variable y, act as guides for which direction the correction is to be done\n",
    "  - The data point denoted by x, specifies the magnitude by which the correction is to be made.\n",
    "  \n",
    " Generic batch Gradient Descent algorithm:\n",
    "  - i  = i  (positive/Negative Number)\n",
    "  - x here acts as the alpha and y as the positive/negative number\n",
    "   \n",
    "\n",
    "- __What happens when the data is not linearly separable? What can be done to salvage the situation?__\n",
    "\n",
    "Answer: When the data is not linear, it is possible that we end up with a poor classification using this method. One possible solution is to increase the number of degrees to fit a more complex line that explains the data better.\n",
    "  Alternative solutions include using a different model that is capable of handling more complexity like an SVM or a Decision Tree Classfication"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "A1-F19.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
